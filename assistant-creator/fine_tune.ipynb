{"cells":[{"cell_type":"markdown","metadata":{"id":"DYA_JIYQgiVv"},"source":["# Data Assistant\n","\n","This is my thesis on creating a large language model that can generate GraphQL queries based on a given prompt.\n","\n","**Why?**\n","\n","By teaching a model to help us reach the information we are looking for, instead of expecting it to have all the information, we can make sure that the answers we get are always up to date and are not limited by a dataset.\n","\n","\n","## Version 1\n","\n","This was a proof of concept gpt model with a really specific tokenizer, that was entirely created and trained from the ground up.\n","\n","The tokenizer specified multiple class of tokens:\n","- 0: END token, describing the end of a prompt to stop generating words.\n","- 1: EMPTY token, used for padding the fixed input size.\n","- 2-7: ARGUMENT tokens, used for passing keywords from the prompt to the output without needing to teach the word to the model.\n","- 8-157: GENERIC tokens, every syntactically required token that doesn't have a special meaning.\n","- 158 and up: GLOSSARY tokens, these are tokens from the rows of the used database which get replaced with the table's name.\n","\n","Using the class's (or table's) name instead of the instance (or row) has interesting pros and cons that allow this type of approach to a model to be very useful in specific scenarios.\n","\n","For example, by only having to teach the model the class names, the results will have better accuracy and less hallucinations; But the model will also require specific training data that, due to the structure of the database it will use, always has to be completely unique, because even the tokenizer depends on it.\n","\n","Using this technique showed promising results with a small sample set, but collecting that specific dataset proved to be too much work.\n","\n","## Version 2\n","\n","To make this project more available, the main dataset needs to be more generalized, so that we can use a really small dataset to fine tune our model.\n","\n","For this we can use the HuggingFace Transformers library, that can specify a tokenizer and makes fine tuning existing models really easy.\n","\n","Install dependencies:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41627,"status":"ok","timestamp":1695132714755,"user":{"displayName":"Áron Wittmann","userId":"11745375133261481543"},"user_tz":-120},"id":"2VhUcf3qB-co","outputId":"dbdc0370-079f-45b1-918d-a1349a6f6c22"},"outputs":[],"source":["pip install pandas; pip install tensorflow; pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"YXNGpzKhpHVn"},"source":["For the base LLM we can use a generic language model like EleutherAI's gpt-j-6B:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw2ilH4kppEY"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n","tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"]},{"cell_type":"markdown","metadata":{"id":"VvA_irrgptm-"},"source":["Then we have to load our GraphQL dataset and tokenize it to fine tune this model.\n","\n","This will create the generic GraphQL generator model that should be tuned further for real applications."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":3021,"status":"error","timestamp":1695132934614,"user":{"displayName":"Áron Wittmann","userId":"11745375133261481543"},"user_tz":-120},"id":"MLNIE3hx-Sul","outputId":"71bf433f-a9f4-4c67-abe4-5ec1ee2d4bbb"},"outputs":[],"source":["import pandas as pd\n","\n","# Load your dataset\n","dataset = pd.read_csv(\"../data/training_data.csv\", delimiter=';')\n","questions = dataset[\"question\"].tolist()\n","answers = dataset[\"query\"].tolist()\n","\n","inputs = tokenizer(questions, answers, padding=True, truncation=True, return_tensors=\"tf\")\n","\n","input_ids = inputs[\"input_ids\"]\n","attention_mask = inputs[\"attention_mask\"]"]},{"cell_type":"markdown","metadata":{"id":"MctVVRKgqcTe"},"source":["Compile the model and run the fine tuning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pP9YHCc3qVzN"},"outputs":[],"source":["import tensorflow as tf\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","model.compile(optimizer=optimizer, loss=loss_fn)\n","\n","model.fit(\n","    [input_ids, attention_mask],\n","    epochs=3,  # Adjust the number of epochs as needed\n",")"]},{"cell_type":"markdown","metadata":{"id":"dSZfJcV1qw0P"},"source":["Lastly, save the tuned model for further uses."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkbL24S8qT4n"},"outputs":[],"source":["model.save_pretrained(\"general_querygen_model\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOw0bIJjS1J4CNMpFiUaODj","mount_file_id":"19CqK6tq8yie3q2-HU6_x9FfEVyXso3PS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
